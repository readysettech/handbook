---
layout: handbook-page-toc
title: "Scalability Team Goals"
---

## On this page
{:.no_toc .hidden-md .hidden-lg}

- TOC
{:toc .hidden-md .hidden-lg}

## Current Goals

Our goals are based on our [vision statement](../#vision).

We are focused on the following two goals:
1. Understand the scaling risks to the most significant services for GitLab.com
1. 75% of the known scaling bottlenecks have been addressed

The reason for specifying 75% is that there will always be more. If we got to 100%, it means we haven't performed
enough recent investigation. The goals keep each the other in balance.

### 1. Understand the scaling risks to the most significant services for GitLab.com

**How do we measure our progress?**

We measure the percentage of our knowledge of the scaling risks to each service.

In [an issue](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/431), we determined the list of services that
the Scalability team should consider. These services have been listed in the table below.

For each service, we answer three questions
1. Do we know how to identify bottlenecks in this service?
1. Have we identified bottlenecks in this service?
1. Have we raised corrective epics or issues for what we have found?

This is averaged for a total across all services.

|**Service**| **How to identify bottlenecks** | **Observations**|
|-----------|-----------|---------------------------|
| API | | |
| CI Runner | | |
| Elasticsearch | | |
| Git | | |
| Gitaly | | |
| Mailroom | | |
| Pages | | |
| pgbouncer | | |
| Postgres | | |
| Praefect | | |
| Redis | We can use [client-side apdex](https://gitlab.com/gitlab-com/runbooks/-/merge_requests/2542) and [redis operations per second](https://dashboards.gitlab.net/d/general-application-demand/general-application-demand-indicators?viewPanel=4&orgId=1) to understand how the system changes over time. | We found that:<br/>- we did not have adequate visibility into our Redis instances, especially for engineers without production access<br/>- we could easily [future proof our Redis usage to support Redis Cluster](https://docs.gitlab.com/ee/development/redis.html#multi-key-commands)<br/>- [high number of calls to SMEMBERS](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/286) lead to high CPU usage<br/>- some data stored in [Redis persistent would be better stored elsewhere](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/285) (or not at all)<br/><br/>Corrective epics and issues have been raised.<br/><br/>Score = 100%|
| Registry | | |
| Share | | |
| Sidekiq | We can use the [Shard Utilization metrics](https://dashboards.gitlab.net/d/sidekiq-shard-detail/sidekiq-shard-detail?viewPanel=20&orgId=1&from=now-24h&to=now) to identify bottlenecks or overprovisioning.  <br/> <br/>This data is the current response of the provisioned infrastructure to [Application Demand](https://dashboards.gitlab.net/d/general-application-demand/general-application-demand-indicators?viewPanel=3&orgId=1), step-changes are indicators of non-growth-related change, being either a degradation (potentially a new bottleneck) or an enhancement (something has been made more efficient). <br/><br/>Other graphs on the [Shard Detail dashboard](https://dashboards.gitlab.net/d/sidekiq-shard-detail/sidekiq-shard-detail) allow us to investigate any changes and identify which specific jobs have caused any changes/breakages, e.g. the "Inflight Jobs per Queue" panel, and the underlying metrics can be used as well to do further ad-hoc analysis. | We found that:  <br/> - we suffered from worker thread saturation because worker threads were reserved for single queues <br/> - we had urgent and non-urgent jobs in the same queue <br/> - we were processing duplicate jobs because many workers were not idempotent. <br/><br/> Corrective epics and issues have been raised. <br/><br/> Score = 100%|
| Web | | |
|             | | TOTAL = 13% |

### 2. 75% of the known scaling bottlenecks have been addressed

**How do we measure our progress?**

We count the number of epics raised through the method above, and we use the percentage of closed epics as our measure.

|**Service**|**Epics and Issues**|**Percentage Complete**|
|-----------|--------------------|-----------------------|
| API | | |
| CI Runner | | |
| Elasticsearch | | |
| Git | | |
| Gitaly | | |
| Mailroom | | |
| Pages | | |
| pgbouncer | | |
| Postgres | | |
| Praefect | | |
| Redis |  1. [Increased Redis observability and monitoring](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/212) - closed <br/>2. [Redis cluster compatibility for shared state Redis](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/211) - closed <br/>3. [Reduce Redis time spent om fetching branch and tag caches by 50%](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/286) - closed <br/>4. [Triage known heavy usages of Redis with stage groups](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/309) <br/>5. [Redis knowledge sharing](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/265) <br/>6. [Clean up Redis persistent / shared state](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/285) | 50% |
| Registry | | |
| Share | | |
| Sidekiq | 1. [Make authorized_projects meet it's SLO's](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/176) - closed <br/> 2. [Improve performance of Authorized Projects Worker](https://gitlab.com/groups/gitlab-org/-/epics/3343) <br/> 3. [Background jobs processing improvements](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/96) - closed <br/> 4. [Make reactive_caching queue not exceed it's SLI](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/191) - closed <br/> 5. [Move Sidekiq-cluster to core](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/181) - closed <br/> 6. [Make sidekiq jobs idempotent](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/207) - closed | 83% |
| Web | | |
| | | TOTAL = 66% |

